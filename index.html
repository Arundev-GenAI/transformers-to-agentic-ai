<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arundev Vamadevan">

<title>FDP-on Transformers to Agentic AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="fdp_files/libs/clipboard/clipboard.min.js"></script>
<script src="fdp_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="fdp_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="fdp_files/libs/quarto-html/popper.min.js"></script>
<script src="fdp_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="fdp_files/libs/quarto-html/anchor.min.js"></script>
<link href="fdp_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="fdp_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="fdp_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="fdp_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="fdp_files/libs/bootstrap/bootstrap-813c323200a87c37e262811031999de4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#hands-on-workshop-on-foundational-language-models-and-fine-tuning" id="toc-hands-on-workshop-on-foundational-language-models-and-fine-tuning" class="nav-link active" data-scroll-target="#hands-on-workshop-on-foundational-language-models-and-fine-tuning"><span class="header-section-number">1</span> Hands-on Workshop on Foundational Language Models and Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#about-me" id="toc-about-me" class="nav-link" data-scroll-target="#about-me"><span class="header-section-number">1.1</span> <strong>About me</strong></a></li>
  <li><a href="#scan-the-qr-code-to-access-this-file" id="toc-scan-the-qr-code-to-access-this-file" class="nav-link" data-scroll-target="#scan-the-qr-code-to-access-this-file"><span class="header-section-number">1.2</span> üì± <strong>Scan the QR Code to Access This File</strong></a></li>
  </ul></li>
  <li><a href="#workshop-introduction-agenda" id="toc-workshop-introduction-agenda" class="nav-link" data-scroll-target="#workshop-introduction-agenda"><span class="header-section-number">2</span> Workshop Introduction &amp; Agenda</a>
  <ul class="collapse">
  <li><a href="#part-1-foundational-language-models" id="toc-part-1-foundational-language-models" class="nav-link" data-scroll-target="#part-1-foundational-language-models"><span class="header-section-number">2.1</span> Part 1 ‚Äî Foundational Language Models</a>
  <ul class="collapse">
  <li><a href="#models-covered" id="toc-models-covered" class="nav-link" data-scroll-target="#models-covered"><span class="header-section-number">2.1.1</span> Models Covered</a></li>
  <li><a href="#hands-on-part-1" id="toc-hands-on-part-1" class="nav-link" data-scroll-target="#hands-on-part-1"><span class="header-section-number">2.1.2</span> Hands-On (Part 1)</a></li>
  </ul></li>
  <li><a href="#part-2-llm-fine-tuning-techniques" id="toc-part-2-llm-fine-tuning-techniques" class="nav-link" data-scroll-target="#part-2-llm-fine-tuning-techniques"><span class="header-section-number">2.2</span> Part 2 ‚Äî LLM Fine-Tuning Techniques</a>
  <ul class="collapse">
  <li><a href="#techniques-covered" id="toc-techniques-covered" class="nav-link" data-scroll-target="#techniques-covered"><span class="header-section-number">2.2.1</span> Techniques Covered</a></li>
  <li><a href="#hands-on-part-2" id="toc-hands-on-part-2" class="nav-link" data-scroll-target="#hands-on-part-2"><span class="header-section-number">2.2.2</span> Hands-On (Part 2)</a></li>
  </ul></li>
  <li><a href="#workshop-outcome" id="toc-workshop-outcome" class="nav-link" data-scroll-target="#workshop-outcome"><span class="header-section-number">2.3</span> Workshop Outcome</a></li>
  </ul></li>
  <li><a href="#what-is-a-language-model" id="toc-what-is-a-language-model" class="nav-link" data-scroll-target="#what-is-a-language-model"><span class="header-section-number">3</span> What is a Language Model?</a>
  <ul class="collapse">
  <li><a href="#simple-definition" id="toc-simple-definition" class="nav-link" data-scroll-target="#simple-definition"><span class="header-section-number">3.1</span> Simple Definition</a></li>
  <li><a href="#how-a-language-model-is-built" id="toc-how-a-language-model-is-built" class="nav-link" data-scroll-target="#how-a-language-model-is-built"><span class="header-section-number">3.2</span> How a Language Model is Built</a></li>
  <li><a href="#technical-architecture" id="toc-technical-architecture" class="nav-link" data-scroll-target="#technical-architecture"><span class="header-section-number">3.3</span> Technical Architecture</a></li>
  <li><a href="#transformer-architecture" id="toc-transformer-architecture" class="nav-link" data-scroll-target="#transformer-architecture"><span class="header-section-number">3.4</span> Transformer Architecture</a>
  <ul class="collapse">
  <li><a href="#key-ideas-that-make-transformers-powerful" id="toc-key-ideas-that-make-transformers-powerful" class="nav-link" data-scroll-target="#key-ideas-that-make-transformers-powerful"><span class="header-section-number">3.4.1</span> Key Ideas That Make Transformers Powerful</a></li>
  <li><a href="#high-level-structure" id="toc-high-level-structure" class="nav-link" data-scroll-target="#high-level-structure"><span class="header-section-number">3.4.2</span> High-Level Structure</a></li>
  </ul></li>
  <li><a href="#difference-between-lm-slm-llm-and-foundational-language-model" id="toc-difference-between-lm-slm-llm-and-foundational-language-model" class="nav-link" data-scroll-target="#difference-between-lm-slm-llm-and-foundational-language-model"><span class="header-section-number">3.5</span> Difference Between LM, SLM, LLM, and Foundational Language Model</a>
  <ul class="collapse">
  <li><a href="#a-language-model-lm" id="toc-a-language-model-lm" class="nav-link" data-scroll-target="#a-language-model-lm"><span class="header-section-number">3.5.1</span> A) Language Model (LM)</a></li>
  <li><a href="#b-small-language-model-slm" id="toc-b-small-language-model-slm" class="nav-link" data-scroll-target="#b-small-language-model-slm"><span class="header-section-number">3.5.2</span> B) Small Language Model (SLM)</a></li>
  <li><a href="#c-large-language-model-llm" id="toc-c-large-language-model-llm" class="nav-link" data-scroll-target="#c-large-language-model-llm"><span class="header-section-number">3.5.3</span> C) Large Language Model (LLM)</a></li>
  <li><a href="#d-foundational-language-model-flm" id="toc-d-foundational-language-model-flm" class="nav-link" data-scroll-target="#d-foundational-language-model-flm"><span class="header-section-number">3.5.4</span> D) Foundational Language Model (FLM)</a></li>
  </ul></li>
  <li><a href="#categorized-by-use-case-functional-specialization" id="toc-categorized-by-use-case-functional-specialization" class="nav-link" data-scroll-target="#categorized-by-use-case-functional-specialization"><span class="header-section-number">3.6</span> Categorized by USE CASE / FUNCTIONAL SPECIALIZATION</a>
  <ul class="collapse">
  <li><a href="#general-purpose-instruction-models" id="toc-general-purpose-instruction-models" class="nav-link" data-scroll-target="#general-purpose-instruction-models"><span class="header-section-number">3.6.1</span> (1) General-purpose Instruction Models</a></li>
  <li><a href="#chat-optimized-models" id="toc-chat-optimized-models" class="nav-link" data-scroll-target="#chat-optimized-models"><span class="header-section-number">3.6.2</span> (2) Chat-optimized Models</a></li>
  <li><a href="#code-generation-models" id="toc-code-generation-models" class="nav-link" data-scroll-target="#code-generation-models"><span class="header-section-number">3.6.3</span> (3) Code Generation Models</a></li>
  <li><a href="#embedding-models" id="toc-embedding-models" class="nav-link" data-scroll-target="#embedding-models"><span class="header-section-number">3.6.4</span> (4) Embedding Models</a></li>
  <li><a href="#efficiency-focused-small-language-models-slms" id="toc-efficiency-focused-small-language-models-slms" class="nav-link" data-scroll-target="#efficiency-focused-small-language-models-slms"><span class="header-section-number">3.6.5</span> (5) Efficiency-Focused / Small Language Models (SLMs)</a></li>
  </ul></li>
  <li><a href="#practical-session-working-with-llms-using-ollama-hugging-face" id="toc-practical-session-working-with-llms-using-ollama-hugging-face" class="nav-link" data-scroll-target="#practical-session-working-with-llms-using-ollama-hugging-face"><span class="header-section-number">3.7</span> Practical Session: Working with LLMs using Ollama &amp; Hugging Face</a></li>
  <li><a href="#llm-application-using-ollama-python-program" id="toc-llm-application-using-ollama-python-program" class="nav-link" data-scroll-target="#llm-application-using-ollama-python-program"><span class="header-section-number">3.8</span> LLM Application using Ollama (Python Program)</a></li>
  <li><a href="#example-tasks-using-hugging-face-transformers-pipeline" id="toc-example-tasks-using-hugging-face-transformers-pipeline" class="nav-link" data-scroll-target="#example-tasks-using-hugging-face-transformers-pipeline"><span class="header-section-number">3.9</span> Example Tasks using Hugging Face <code>transformers</code> Pipeline</a></li>
  <li><a href="#research-paper-summarizer-using-llms-hugging-face-arxiv-api" id="toc-research-paper-summarizer-using-llms-hugging-face-arxiv-api" class="nav-link" data-scroll-target="#research-paper-summarizer-using-llms-hugging-face-arxiv-api"><span class="header-section-number">3.10</span> Research Paper Summarizer using LLMs (Hugging Face + arXiv API)</a></li>
  <li><a href="#llm-augmentation" id="toc-llm-augmentation" class="nav-link" data-scroll-target="#llm-augmentation"><span class="header-section-number">3.11</span> LLM Augmentation</a>
  <ul class="collapse">
  <li><a href="#techniques-of-llm-augmentation" id="toc-techniques-of-llm-augmentation" class="nav-link" data-scroll-target="#techniques-of-llm-augmentation"><span class="header-section-number">3.11.1</span> Techniques of LLM Augmentation</a></li>
  </ul></li>
  <li><a href="#choosing-the-right-optimization" id="toc-choosing-the-right-optimization" class="nav-link" data-scroll-target="#choosing-the-right-optimization"><span class="header-section-number">3.12</span> Choosing the Right Optimization</a>
  <ul class="collapse">
  <li><a href="#time-is-a-constraint" id="toc-time-is-a-constraint" class="nav-link" data-scroll-target="#time-is-a-constraint"><span class="header-section-number">3.12.1</span> Time is a Constraint</a></li>
  <li><a href="#if-you-need-real-time-data" id="toc-if-you-need-real-time-data" class="nav-link" data-scroll-target="#if-you-need-real-time-data"><span class="header-section-number">3.12.2</span> If You Need Real-Time Data</a></li>
  <li><a href="#if-you-need-high-accuracy-and-efficiency" id="toc-if-you-need-high-accuracy-and-efficiency" class="nav-link" data-scroll-target="#if-you-need-high-accuracy-and-efficiency"><span class="header-section-number">3.12.3</span> If You Need High Accuracy and Efficiency</a></li>
  <li><a href="#best-practice" id="toc-best-practice" class="nav-link" data-scroll-target="#best-practice"><span class="header-section-number">3.12.4</span> Best Practice</a></li>
  </ul></li>
  <li><a href="#training-own-model-for-llm-optimization" id="toc-training-own-model-for-llm-optimization" class="nav-link" data-scroll-target="#training-own-model-for-llm-optimization"><span class="header-section-number">3.13</span> Training Own Model for LLM Optimization</a>
  <ul class="collapse">
  <li><a href="#why-train-from-scratch" id="toc-why-train-from-scratch" class="nav-link" data-scroll-target="#why-train-from-scratch"><span class="header-section-number">3.13.1</span> Why Train from Scratch?</a></li>
  <li><a href="#drawbacks-of-training-from-scratch" id="toc-drawbacks-of-training-from-scratch" class="nav-link" data-scroll-target="#drawbacks-of-training-from-scratch"><span class="header-section-number">3.13.2</span> Drawbacks of Training from Scratch</a></li>
  <li><a href="#fine-tuning-llms" id="toc-fine-tuning-llms" class="nav-link" data-scroll-target="#fine-tuning-llms"><span class="header-section-number">3.13.3</span> Fine-tuning LLMs</a></li>
  </ul></li>
  <li><a href="#key-benefits-of-fine-tuning" id="toc-key-benefits-of-fine-tuning" class="nav-link" data-scroll-target="#key-benefits-of-fine-tuning"><span class="header-section-number">3.14</span> Key Benefits of Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#domain-adaptation" id="toc-domain-adaptation" class="nav-link" data-scroll-target="#domain-adaptation"><span class="header-section-number">3.14.1</span> Domain Adaptation</a></li>
  <li><a href="#enhanced-performance-in-niche-tasks" id="toc-enhanced-performance-in-niche-tasks" class="nav-link" data-scroll-target="#enhanced-performance-in-niche-tasks"><span class="header-section-number">3.14.2</span> Enhanced Performance in Niche Tasks</a></li>
  <li><a href="#cost-optimization" id="toc-cost-optimization" class="nav-link" data-scroll-target="#cost-optimization"><span class="header-section-number">3.14.3</span> Cost Optimization</a></li>
  <li><a href="#workflow-alignment" id="toc-workflow-alignment" class="nav-link" data-scroll-target="#workflow-alignment"><span class="header-section-number">3.14.4</span> Workflow Alignment</a></li>
  </ul></li>
  <li><a href="#types-of-fine-tuning" id="toc-types-of-fine-tuning" class="nav-link" data-scroll-target="#types-of-fine-tuning"><span class="header-section-number">3.15</span> Types of Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#full-fine-tuning" id="toc-full-fine-tuning" class="nav-link" data-scroll-target="#full-fine-tuning"><span class="header-section-number">3.15.1</span> 1 Full Fine-Tuning</a></li>
  <li><a href="#lora-low-rank-adaptation" id="toc-lora-low-rank-adaptation" class="nav-link" data-scroll-target="#lora-low-rank-adaptation"><span class="header-section-number">3.15.2</span> 2Ô∏è LoRA (Low-Rank Adaptation)</a></li>
  <li><a href="#qlora-quantized-lora" id="toc-qlora-quantized-lora" class="nav-link" data-scroll-target="#qlora-quantized-lora"><span class="header-section-number">3.15.3</span> 3Ô∏è QLoRA (Quantized LoRA)</a></li>
  <li><a href="#prefix-tuning-or-p-tuning" id="toc-prefix-tuning-or-p-tuning" class="nav-link" data-scroll-target="#prefix-tuning-or-p-tuning"><span class="header-section-number">3.15.4</span> 4Ô∏è Prefix Tuning (or P-Tuning)</a></li>
  </ul></li>
  <li><a href="#full-fine-tuning-implementation" id="toc-full-fine-tuning-implementation" class="nav-link" data-scroll-target="#full-fine-tuning-implementation"><span class="header-section-number">3.16</span> Full Fine-Tuning Implementation</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">FDP-on Transformers to Agentic AI</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Arundev Vamadevan <a href="mailto:Arundev.Vamadevan@ncirl.ie" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Associate Faculty Lecturer, National College of Ireland
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<style>

body::before,
.quarto-body::before,
.main::before {
  content: "CONFIDENTIAL - FOR FDP PARTICIPANTS ONLY" !important;
  position: fixed !important;
  top: 50% !important;
  left: 50% !important;
  transform: translate(-50%,-50%) rotate(-45deg) !important;
  font-size: 3.2rem !important;
  line-height: 1 !important;
  letter-spacing: 0.05em !important;
  color: rgba(0,0,0,0.10) !important;
  text-transform: uppercase !important;
  pointer-events: none !important;
  z-index: 2147483646 !important; /* very high to stay on top */
  white-space: nowrap !important;
  text-align: center !important;
  display: block !important;
  -webkit-user-select: none !important;
  user-select: none !important;
}

/* Slightly smaller on mobile */
@media (max-width: 700px){
  body::before,
  .quarto-body::before,
  .main::before {
    font-size: 2.0rem !important;
    color: rgba(0,0,0,0.08) !important;
  }
}

/* Footer: fixed at bottom, subtle background so content readable */
footer.qmd-watermark-footer{
  width:100% !important;
  text-align:center !important;
  font-size:0.9em !important;
  color: #555 !important;
  position: fixed !important;
  bottom: 0 !important;
  left: 0 !important;
  background: rgba(255,255,255,0.94) !important;
  padding: 0.45em 0 !important;
  z-index: 2147483647 !important;
  box-shadow: 0 -1px 6px rgba(0,0,0,0.06) !important;
}

/* Ensure main content is above base stacking so watermark doesn't block interactive elements */
main, .page, .quarto-body, .quarto-content {
  position: relative !important;
  z-index: 1 !important;
}

/* Avoid footer overlapping floating UI elements ‚Äî add a small bottom padding to page content */
.quarto-body, .page, main {
  padding-bottom: 3.2rem !important; /* adjust if your footer is larger */
}
</style>
<section id="hands-on-workshop-on-foundational-language-models-and-fine-tuning" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Hands-on Workshop on Foundational Language Models and Fine-Tuning</h1>
<p>Welcome to this FDP program.<br>
You can also access this file online using the QR code below.</p>
<hr>
<section id="about-me" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="about-me"><span class="header-section-number">1.1</span> <strong>About me</strong></h2>
<p><strong>Name:</strong> Arundev Vamadevan<br>
<strong>Designation:</strong> Associate Faculty Lecturer and Research Supervisor, National College of Ireland, Dublin.<br>
<strong>Email:</strong> <a href="mailto:Arundev.Vamadevan@ncirl.ie" class="email">Arundev.Vamadevan@ncirl.ie</a><br>
<strong>LinkedIn:</strong> <a href="https://www.linkedin.com/in/arundev-v">LinkedIn Profile</a></p>
<hr>
</section>
<section id="scan-the-qr-code-to-access-this-file" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="scan-the-qr-code-to-access-this-file"><span class="header-section-number">1.2</span> üì± <strong>Scan the QR Code to Access This File</strong></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/qrcode.png" class="img-fluid figure-img"></p>
<figcaption>QR Code</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="workshop-introduction-agenda" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Workshop Introduction &amp; Agenda</h1>
<p>Welcome to the Hands-On Workshop on Modern Language Models &amp; Fine-Tuning Techniques.<br>
In this session, we will explore the latest advancements in Foundational Language Models (FLMs) and learn how to fine-tune LLMs efficiently, both theoretically and practically.</p>
<p>This workshop is designed to give you:</p>
<p>‚úî A clear understanding of how today‚Äôs leading language models work<br>
‚úî Hands-on experience running these models locally using Ollama and Hugging Face<br>
‚úî Practical knowledge of parameter-efficient fine-tuning, including LoRA and P-Tuning<br>
‚úî End-to-end workflows for inference, embeddings, and fine-tuning<br>
‚úî Real-world coding examples you can reuse in your research or projects</p>
<hr>
<section id="part-1-foundational-language-models" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="part-1-foundational-language-models"><span class="header-section-number">2.1</span> Part 1 ‚Äî Foundational Language Models</h2>
<p>We begin by understanding what Foundational Language Models (FLMs) are and how they differ from regular Large Language Models (LLMs).<br>
We will explore different categories of FLMs based on:</p>
<ul>
<li>Model size (small, medium, large)</li>
<li>Use case (chat, coding, embeddings)</li>
<li>Architecture (Mixture-of-Experts, decoder-only transformers, small language models)</li>
<li>Local usage (Ollama, CPU/GPU compatibility)</li>
</ul>
<section id="models-covered" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="models-covered"><span class="header-section-number">2.1.1</span> Models Covered</h3>
<p>üîπ <strong>LLaMA 3 Family</strong></p>
<ul>
<li>LLaMA 3 Instruct 8B / 70B ‚Äî instruction-tuned general-purpose models<br>
</li>
<li>LLaMA 3 Chat ‚Äî conversational AI models<br>
</li>
<li>LLaMA 3 Code ‚Äî optimized for programming tasks</li>
</ul>
<p>üîπ <strong>Embedding Model</strong></p>
<ul>
<li>E5 Embedding Models ‚Äî high-quality embeddings for RAG, clustering, similarity search</li>
</ul>
<p>üîπ <strong>Mixture-of-Experts (MoE)</strong></p>
<ul>
<li>Mixtral 8x7B ‚Äî sparse MoE architecture for high-performance inference</li>
</ul>
<p>üîπ <strong>Small Language Models (SLMs)</strong></p>
<ul>
<li>PHI-3 ‚Äî compact, high-efficiency SLM suitable for edge devices<br>
</li>
<li>BitNet B1.58 ‚Äî bit-based efficient transformer for ultra-low-resource environments</li>
</ul>
<hr>
</section>
<section id="hands-on-part-1" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="hands-on-part-1"><span class="header-section-number">2.1.2</span> Hands-On (Part 1)</h3>
<ul>
<li>Running LLaMA 3, Mixtral, PHI-3 locally using Ollama<br>
</li>
<li>Using E5 models with Hugging Face Sentence Transformers<br>
</li>
<li>Chat generation, coding tasks, document understanding<br>
</li>
<li>Building a small RAG pipeline<br>
</li>
<li>Comparing performance of SLMs vs LLMs</li>
</ul>
<hr>
</section>
</section>
<section id="part-2-llm-fine-tuning-techniques" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="part-2-llm-fine-tuning-techniques"><span class="header-section-number">2.2</span> Part 2 ‚Äî LLM Fine-Tuning Techniques</h2>
<p>In the second half, we dive into how to adapt large models for specific tasks using modern, efficient techniques.</p>
<p>We explore:</p>
<ul>
<li>Why full fine-tuning is expensive<br>
</li>
<li>How parameter-efficient methods reduce cost by 10‚Äì100√ó<br>
</li>
<li>Practical implementation using Hugging Face Transformers and PEFT</li>
</ul>
<section id="techniques-covered" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="techniques-covered"><span class="header-section-number">2.2.1</span> Techniques Covered</h3>
<p>üîπ <strong>Parameter-Efficient Fine-Tuning (PEFT)</strong><br>
A framework enabling efficient adaptation of large models with minimal trainable weights.</p>
<p>üîπ <strong>LoRA (Low-Rank Adaptation)</strong><br>
The most widely used PEFT technique‚Äîfast, memory-efficient, and production ready.</p>
<p>üîπ <strong>P-Tuning / Prompt Tuning</strong><br>
Train soft prompts instead of model weights; lightweight but powerful.</p>
<p>üîπ <strong>Full Fine-Tuning (optional)</strong><br>
Understand the traditional approach‚Äîuseful for research contexts.</p>
<p>üîπ <strong>Fine-Tuning Embedding Models</strong><br>
How to customize embedding representation for domain-specific retrieval tasks.</p>
<hr>
</section>
<section id="hands-on-part-2" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="hands-on-part-2"><span class="header-section-number">2.2.2</span> Hands-On (Part 2)</h3>
<ul>
<li>Implement LoRA fine-tuning on a lightweight LLM<br>
</li>
<li>Compare LoRA vs Full Fine-Tuning<br>
</li>
<li>Parameter-efficient training using Hugging Face PEFT<br>
</li>
<li>Fine-tuning a small embedding model (E5 or BERT-based)<br>
</li>
<li>Running the fine-tuned model locally on Ollama (via GGUF)<br>
</li>
<li>Evaluation: perplexity, accuracy, semantic similarity</li>
</ul>
<hr>
</section>
</section>
<section id="workshop-outcome" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="workshop-outcome"><span class="header-section-number">2.3</span> Workshop Outcome</h2>
<p>By the end of the workshop, participants will:</p>
<p>‚úî Understand the landscape of modern language models<br>
‚úî Know how to run LLMs locally on laptops (via Ollama + HF)<br>
‚úî Be able to perform inference, embeddings, and RAG<br>
‚úî Know how to fine-tune LLMs using LoRA, P-Tuning, and PEFT<br>
‚úî Gain reusable code for research and development<br>
‚úî Build confidence in working with state-of-the-art LLM techniques</p>
</section>
</section>
<section id="what-is-a-language-model" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> What is a Language Model?</h1>
<section id="simple-definition" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="simple-definition"><span class="header-section-number">3.1</span> Simple Definition</h2>
<p>A Language Model (LM) is a computer program that learns patterns in language and can predict or generate text‚Äîjust like how humans guess the next word in a sentence.</p>
<p><strong>Example:</strong><br>
‚ÄúI am going to the ‚Ä¶‚Äù ‚Üí a language model may predict ‚Äústore‚Äù, ‚Äúpark‚Äù, ‚Äúgym‚Äù, etc.</p>
<hr>
</section>
<section id="how-a-language-model-is-built" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="how-a-language-model-is-built"><span class="header-section-number">3.2</span> How a Language Model is Built</h2>
<p><strong>1. Collect text data</strong><br>
The model is trained on books, articles, websites, chats, code, etc.</p>
<p><strong>2. Convert text into numbers</strong><br>
Words/sentences get converted into vectors (numbers) using embeddings.</p>
<p><strong>3. Train a neural network</strong><br>
A transformer-based network learns:</p>
<ul>
<li>grammar<br>
</li>
<li>meaning<br>
</li>
<li>reasoning patterns<br>
</li>
<li>next-word predictions</li>
</ul>
<p><strong>4. The model improves by seeing millions/billions of examples</strong><br>
It learns how humans use language.</p>
<hr>
</section>
<section id="technical-architecture" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="technical-architecture"><span class="header-section-number">3.3</span> Technical Architecture</h2>
<p>Most modern language models use a structure called <strong>Transformer</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Transformers.png" class="img-fluid figure-img"></p>
<figcaption>Transformer Architecture</figcaption>
</figure>
</div>
<p><strong>Source:</strong> <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p>
</section>
<section id="transformer-architecture" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="transformer-architecture"><span class="header-section-number">3.4</span> Transformer Architecture</h2>
<p>The Transformer Architecture, introduced in the 2017 <em>Attention Is All You Need</em> paper, changed how we build large language models. Unlike earlier NLP models that process text one word at a time, transformers can look at <strong>all the words in a sentence simultaneously</strong>. This is possible because of <strong>attention</strong>, which helps the model understand how each word relates to every other word, regardless of position.</p>
<p>During training, the model learns <strong>attention weights</strong>‚Äîscores that indicate which words are more important when making predictions. Because transformers process words in parallel instead of sequentially, they are significantly faster and more efficient.</p>
<section id="key-ideas-that-make-transformers-powerful" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="key-ideas-that-make-transformers-powerful"><span class="header-section-number">3.4.1</span> Key Ideas That Make Transformers Powerful</h3>
<ul>
<li><p><strong>Self-Attention</strong><br>
Helps the model understand how each word connects to all other words in the sentence. Over time, the model forms an internal understanding of language.</p></li>
<li><p><strong>Positional Encoding</strong><br>
Since transformers don‚Äôt naturally read text in order, positional encoding adds information about word order to the input, helping the model interpret sequence structure.</p></li>
</ul>
<p>Transformers are also considered <strong>semi-supervised models</strong>, as they are first pre-trained on massive unlabelled datasets and later fine-tuned using labelled data.</p>
</section>
<section id="high-level-structure" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="high-level-structure"><span class="header-section-number">3.4.2</span> High-Level Structure</h3>
<p>Transformers consist of two major components:</p>
<ul>
<li><strong>Encoder</strong><br>
</li>
<li><strong>Decoder</strong></li>
</ul>
<p>Both rely on shared components such as tokenisation, embeddings, positional encodings, self-attention layers, and feed-forward networks.</p>
</section>
</section>
<section id="difference-between-lm-slm-llm-and-foundational-language-model" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="difference-between-lm-slm-llm-and-foundational-language-model"><span class="header-section-number">3.5</span> Difference Between LM, SLM, LLM, and Foundational Language Model</h2>
<p>Below is a very simple, beginner-friendly explanation of the different types of language models.</p>
<section id="a-language-model-lm" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="a-language-model-lm"><span class="header-section-number">3.5.1</span> A) Language Model (LM)</h3>
<p>A <strong>Language Model (LM)</strong> is a general term for any model that can understand or generate human language.</p>
<p><strong>Key Points:</strong> - Can be <em>small or large</em> - Can use old architectures (RNN, LSTM) or modern ones (Transformers) - Used for predicting or generating text</p>
<p><strong>Examples:</strong> GPT-2, BERT, GPT-Neo</p>
<hr>
</section>
<section id="b-small-language-model-slm" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="b-small-language-model-slm"><span class="header-section-number">3.5.2</span> B) Small Language Model (SLM)</h3>
<p>An <strong>SLM</strong> is a <em>compact</em> language model designed for efficiency.</p>
<p><strong>Key Points:</strong> - Can run on smaller devices (laptops, phones, Raspberry Pi) - Uses fewer parameters (usually &lt; 1B‚Äì3B) - Fast, efficient, and cheaper to train - Lower reasoning ability than LLMs</p>
<p><strong>Examples:</strong> - PHI-3<br>
- BitNet B1.58<br>
- LLaMA 3.2 1B<br>
- Gemma 2B</p>
<hr>
</section>
<section id="c-large-language-model-llm" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="c-large-language-model-llm"><span class="header-section-number">3.5.3</span> C) Large Language Model (LLM)</h3>
<p>An <strong>LLM</strong> is a <em>large-scale</em> model trained on massive datasets.</p>
<p><strong>Features:</strong> - Strong reasoning and problem-solving ability<br>
- High-quality text generation<br>
- Supports complex tasks (coding, math, planning)<br>
- Typically 7B‚Äì70B+ parameters</p>
<p><strong>Examples:</strong> - LLaMA 3 8B / 70B<br>
- Mixtral 8x7B<br>
- GPT-3.5 / GPT-4<br>
- Claude 3</p>
<hr>
</section>
<section id="d-foundational-language-model-flm" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="d-foundational-language-model-flm"><span class="header-section-number">3.5.4</span> D) Foundational Language Model (FLM)</h3>
<p>A <strong>Foundational Model</strong> is a very large, general-purpose model trained on huge and diverse datasets that can be adapted (fine-tuned) for many downstream tasks.</p>
<p><strong>Key Points:</strong> - Broad, general-purpose capabilities - Acts as a base model for specialized versions - Usually trained on multimodal or extremely large datasets</p>
<p><strong>Examples:</strong> GPT-4, LLaMA-3, Claude Opus, Gemini 1.5</p>
<hr>
</section>
</section>
<section id="categorized-by-use-case-functional-specialization" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="categorized-by-use-case-functional-specialization"><span class="header-section-number">3.6</span> Categorized by USE CASE / FUNCTIONAL SPECIALIZATION</h2>
<section id="general-purpose-instruction-models" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="general-purpose-instruction-models"><span class="header-section-number">3.6.1</span> (1) General-purpose Instruction Models</h3>
<p>Designed for reasoning, answering queries, and conversational AI.</p>
<p><strong>Examples:</strong> - LLaMA 3 Instruct 8B<br>
- LLaMA 3 Instruct 70B<br>
- Mixtral 8x7B<br>
- PHI-3<br>
- BitNet B1.58</p>
<hr>
</section>
<section id="chat-optimized-models" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="chat-optimized-models"><span class="header-section-number">3.6.2</span> (2) Chat-optimized Models</h3>
<p>Built specifically for dialog, memory, and multi-turn conversation.</p>
<p><strong>Examples:</strong> - LLaMA 3 Chat 8B<br>
- LLaMA 3 Chat 70B<br>
- PHI-3 Chat versions</p>
<hr>
</section>
<section id="code-generation-models" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="code-generation-models"><span class="header-section-number">3.6.3</span> (3) Code Generation Models</h3>
<p>Optimized for coding, debugging, and code completion in languages like Python, JavaScript, and Java.</p>
<p><strong>Examples:</strong> - LLaMA 3 Code 8B<br>
- LLaMA 3 Code 70B</p>
<hr>
</section>
<section id="embedding-models" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="embedding-models"><span class="header-section-number">3.6.4</span> (4) Embedding Models</h3>
<p>Used for Retrieval-Augmented Generation (RAG), semantic search, and vector databases.</p>
<p><strong>Examples:</strong> - E5 Embedding Model Family<br>
- e5-small<br>
- e5-base<br>
- e5-large</p>
<hr>
</section>
<section id="efficiency-focused-small-language-models-slms" class="level3" data-number="3.6.5">
<h3 data-number="3.6.5" class="anchored" data-anchor-id="efficiency-focused-small-language-models-slms"><span class="header-section-number">3.6.5</span> (5) Efficiency-Focused / Small Language Models (SLMs)</h3>
<p>Used for low-power devices or edge deployment.</p>
<p><strong>Examples:</strong> - PHI-3<br>
- BitNet B1.58 (binary-weight efficient model)</p>
</section>
</section>
<section id="practical-session-working-with-llms-using-ollama-hugging-face" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="practical-session-working-with-llms-using-ollama-hugging-face"><span class="header-section-number">3.7</span> Practical Session: Working with LLMs using Ollama &amp; Hugging Face</h2>
<p>In this hands-on session, we will explore how to run and experiment with modern Language Models both <strong>locally</strong> (using Ollama) and <strong>from the cloud</strong> (using Hugging Face).</p>
</section>
<section id="llm-application-using-ollama-python-program" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="llm-application-using-ollama-python-program"><span class="header-section-number">3.8</span> LLM Application using Ollama (Python Program)</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_text(promt):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> ollama.generate(model<span class="op">=</span><span class="st">'llama2'</span>, prompt<span class="op">=</span>promt)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result[<span class="st">'response'</span>]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> question_answer(question):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f'Answer the question concisely : </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> generate_text(prompt)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> summarize(paragraph):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f'summarize the below paragraph into 2 sentence: </span><span class="sc">{</span>paragraph<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> generate_text(prompt)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lang_translate(sentence, language):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f'convert the below sentence </span><span class="sc">{</span>sentence<span class="sc">}</span><span class="ss"> in to </span><span class="sc">{</span>language<span class="sc">}</span><span class="ss"> language'</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> generate_text(prompt)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Application NLP Tasks"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"1. Generate Text"</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"2. Question Answring"</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"3. Summary of a paragraph"</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"4. Language Translation "</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Enter your choice"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>choice <span class="op">=</span> <span class="bu">input</span>(<span class="st">"1/2/3/4"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> choice <span class="op">==</span> <span class="st">'1'</span>:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="bu">input</span>(<span class="st">"Enter your prompt here"</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> generate_text(prompt)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> choice <span class="op">==</span> <span class="st">'2'</span>:</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    question <span class="op">=</span> <span class="bu">input</span>(<span class="st">"Enter your question here"</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> question_answer(question)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> choice <span class="op">==</span> <span class="st">'3'</span>:</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    paragraph <span class="op">=</span> <span class="bu">input</span>(<span class="st">"Enter the paragraph to summarize"</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> summarize(paragraph)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> choice <span class="op">==</span> <span class="st">'4'</span>:</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    sentence <span class="op">=</span> <span class="bu">input</span>(<span class="st">"Enter the sentence to translate"</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    language <span class="op">=</span> <span class="bu">input</span>(<span class="st">"Enter the language"</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> lang_translate(sentence, language)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> <span class="st">"Wrong Choice !!!"</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Result : "</span>, result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="example-tasks-using-hugging-face-transformers-pipeline" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="example-tasks-using-hugging-face-transformers-pipeline"><span class="header-section-number">3.9</span> Example Tasks using Hugging Face <code>transformers</code> Pipeline</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Task 1: Sentiment Analysis</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> classifier(<span class="st">"I was happy to visit my home country"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Task 2: Text Generation</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"distilbert/distilgpt2"</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> text_generator(</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Today is a rainy day in London"</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    num_return_sequences<span class="op">=</span><span class="dv">2</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated_text:</span><span class="ch">\n</span><span class="st">"</span>, generated_text)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Task 3: Question Answering</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>qa_model <span class="op">=</span> pipeline(<span class="st">"question-answering"</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is my job?"</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"I am developing AI models with Python."</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(qa_model(question<span class="op">=</span>question, context<span class="op">=</span>context))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="research-paper-summarizer-using-llms-hugging-face-arxiv-api" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="research-paper-summarizer-using-llms-hugging-face-arxiv-api"><span class="header-section-number">3.10</span> Research Paper Summarizer using LLMs (Hugging Face + arXiv API)</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arxiv</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Query arXiv for AI / ML papers</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">'ai OR artificial intelligence OR machine learning'</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>search <span class="op">=</span> arxiv.Search(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    query<span class="op">=</span>query,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    max_results<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    sort_by<span class="op">=</span>arxiv.SortCriterion.SubmittedDate</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract metadata from results</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>papers <span class="op">=</span> []</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> result <span class="kw">in</span> search.results():</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    papers.append({</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'published'</span>: result.published,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">'title'</span>: result.title,</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">'abstract'</span>: result.summary,</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">'categories'</span>: result.categories</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(papers)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_colwidth'</span>, <span class="va">None</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first 10 papers</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">10</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: take first abstract</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>abstract <span class="op">=</span> df[<span class="st">'abstract'</span>][<span class="dv">0</span>]</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Load summarization model</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>, model<span class="op">=</span><span class="st">"facebook/bart-large-cnn"</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize abstract</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>summarization_result <span class="op">=</span> summarizer(abstract)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>summarization_result[<span class="dv">0</span>][<span class="st">'summary_text'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="llm-augmentation" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="llm-augmentation"><span class="header-section-number">3.11</span> LLM Augmentation</h2>
<p>LLM (Large Language Model) Augmentation refers to techniques used to enhance the capabilities of large language models by integrating external data sources, retrieval mechanisms, fine-tuning strategies, or computational methods to improve their accuracy, relevance, and contextual understanding.</p>
<section id="techniques-of-llm-augmentation" class="level3" data-number="3.11.1">
<h3 data-number="3.11.1" class="anchored" data-anchor-id="techniques-of-llm-augmentation"><span class="header-section-number">3.11.1</span> Techniques of LLM Augmentation</h3>
<section id="retrieval-augmented-generation-rag" class="level4" data-number="3.11.1.1">
<h4 data-number="3.11.1.1" class="anchored" data-anchor-id="retrieval-augmented-generation-rag"><span class="header-section-number">3.11.1.1</span> Retrieval-Augmented Generation (RAG)</h4>
<ul>
<li>Combines LLMs with an external knowledge base to fetch relevant information before generating a response.</li>
</ul>
</section>
<section id="fine-tuning-parameter-efficient-fine-tuning-peft" class="level4" data-number="3.11.1.2">
<h4 data-number="3.11.1.2" class="anchored" data-anchor-id="fine-tuning-parameter-efficient-fine-tuning-peft"><span class="header-section-number">3.11.1.2</span> Fine-Tuning &amp; Parameter Efficient Fine-Tuning (PEFT)</h4>
<ul>
<li>Fine-tuning the model using domain-specific datasets to improve its performance in a particular task.</li>
</ul>
<p><strong>Techniques:</strong> - Full fine-tuning - LoRA (Low-Rank Adaptation) - QLoRA (Quantized LoRA) - Prefix Tuning</p>
</section>
<section id="prompt-engineering-prompt-tuning" class="level4" data-number="3.11.1.3">
<h4 data-number="3.11.1.3" class="anchored" data-anchor-id="prompt-engineering-prompt-tuning"><span class="header-section-number">3.11.1.3</span> Prompt Engineering &amp; Prompt Tuning</h4>
<ul>
<li>Optimizing input prompts to get more accurate and relevant responses from the model.</li>
</ul>
<p><strong>Techniques:</strong> - Zero-shot, Few-shot, and Chain-of-Thought (CoT) prompting - Self-consistency prompting - Prompt tuning with soft tokens</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/6_Prompt_Engineering.png" class="img-fluid figure-img"></p>
<figcaption>LLM Augmentation</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="choosing-the-right-optimization" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="choosing-the-right-optimization"><span class="header-section-number">3.12</span> Choosing the Right Optimization</h2>
<section id="time-is-a-constraint" class="level3" data-number="3.12.1">
<h3 data-number="3.12.1" class="anchored" data-anchor-id="time-is-a-constraint"><span class="header-section-number">3.12.1</span> Time is a Constraint</h3>
<ul>
<li>Start with <strong>Prompt Engineering</strong> ‚Äî it‚Äôs fast, easy to experiment with, and requires no model changes.</li>
</ul>
</section>
<section id="if-you-need-real-time-data" class="level3" data-number="3.12.2">
<h3 data-number="3.12.2" class="anchored" data-anchor-id="if-you-need-real-time-data"><span class="header-section-number">3.12.2</span> If You Need Real-Time Data</h3>
<ul>
<li>Use <strong>RAG (Retrieval-Augmented Generation)</strong> to fetch external or frequently updated information, ideal for apps like:
<ul>
<li>Customer support</li>
<li>Semantic search</li>
<li>Financial dashboards</li>
</ul></li>
</ul>
</section>
<section id="if-you-need-high-accuracy-and-efficiency" class="level3" data-number="3.12.3">
<h3 data-number="3.12.3" class="anchored" data-anchor-id="if-you-need-high-accuracy-and-efficiency"><span class="header-section-number">3.12.3</span> If You Need High Accuracy and Efficiency</h3>
<ul>
<li><strong>Fine-tune the model</strong> when you want to reduce latency and token costs for well-defined, repetitive tasks like:
<ul>
<li>Legal compliance checks</li>
<li>Automated reporting</li>
</ul></li>
</ul>
</section>
<section id="best-practice" class="level3" data-number="3.12.4">
<h3 data-number="3.12.4" class="anchored" data-anchor-id="best-practice"><span class="header-section-number">3.12.4</span> Best Practice</h3>
<ul>
<li>The best practice is to <strong>combine all three techniques</strong> to get the most out of your LLM:
<ul>
<li><strong>Prompt Engineering</strong> helps you prototype and test prompts quickly</li>
<li><strong>RAG</strong> ensures that your model has access to the latest external information</li>
<li><strong>Fine-Tuning</strong> makes the model more efficient for domain-specific tasks by reducing token usage and improving accuracy</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/9_Comparison.png" class="img-fluid figure-img"></p>
<figcaption>Best Practise</figcaption>
</figure>
</div>
<p><a href="https://platform.openai.com/docs/guides/optimizing-llm-accuracy">Source : Optimizing LLM Accuracy ‚Äì OpenAI Documentation</a></p>
</section>
</section>
<section id="training-own-model-for-llm-optimization" class="level2" data-number="3.13">
<h2 data-number="3.13" class="anchored" data-anchor-id="training-own-model-for-llm-optimization"><span class="header-section-number">3.13</span> Training Own Model for LLM Optimization</h2>
<section id="why-train-from-scratch" class="level3" data-number="3.13.1">
<h3 data-number="3.13.1" class="anchored" data-anchor-id="why-train-from-scratch"><span class="header-section-number">3.13.1</span> Why Train from Scratch?</h3>
<ul>
<li>If you need <strong>deep, specific domain knowledge</strong> without any bias from pre-existing models</li>
<li>If your <strong>data or the use case is unique</strong>, this approach provides more precise and controlled outcomes</li>
<li>In <strong>sensitive industries</strong>, having a model trained entirely on internal data can help meet stringent compliance requirements</li>
</ul>
</section>
<section id="drawbacks-of-training-from-scratch" class="level3" data-number="3.13.2">
<h3 data-number="3.13.2" class="anchored" data-anchor-id="drawbacks-of-training-from-scratch"><span class="header-section-number">3.13.2</span> Drawbacks of Training from Scratch</h3>
<ul>
<li>Requires substantial <strong>computational power, storage</strong>, and sometimes <strong>weeks to complete</strong></li>
<li><strong>High-quality and large-scale data</strong> is essential, which can be costly and time-consuming to collect</li>
<li>Need <strong>specialized skills</strong> in deep learning, hyperparameter tuning, and model optimization</li>
<li>This process is <strong>lengthy</strong> compared to fine-tuning or prompt engineering</li>
</ul>
</section>
<section id="fine-tuning-llms" class="level3" data-number="3.13.3">
<h3 data-number="3.13.3" class="anchored" data-anchor-id="fine-tuning-llms"><span class="header-section-number">3.13.3</span> Fine-tuning LLMs</h3>
<ul>
<li><strong>Fine-tuning</strong> is the process of taking a <strong>pre-trained machine learning model</strong> and further training it on a <strong>specific dataset</strong> to adapt it for a particular task or domain.</li>
<li>The model is already trained on large amounts of data but needs to be <strong>optimized for domain-specific knowledge</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/10_fine_tuning.png" class="img-fluid figure-img"></p>
<figcaption>Fine-Tuning</figcaption>
</figure>
</div>
</section>
</section>
<section id="key-benefits-of-fine-tuning" class="level2" data-number="3.14">
<h2 data-number="3.14" class="anchored" data-anchor-id="key-benefits-of-fine-tuning"><span class="header-section-number">3.14</span> Key Benefits of Fine-Tuning</h2>
<section id="domain-adaptation" class="level3" data-number="3.14.1">
<h3 data-number="3.14.1" class="anchored" data-anchor-id="domain-adaptation"><span class="header-section-number">3.14.1</span> Domain Adaptation</h3>
<ul>
<li>Pre-trained models do not inherently understand <strong>proprietary information</strong> or <strong>specialized terminology</strong>.</li>
<li>Example: They won‚Äôt recognize your company‚Äôs latest product features unless fine-tuned on relevant documentation and data.</li>
</ul>
</section>
<section id="enhanced-performance-in-niche-tasks" class="level3" data-number="3.14.2">
<h3 data-number="3.14.2" class="anchored" data-anchor-id="enhanced-performance-in-niche-tasks"><span class="header-section-number">3.14.2</span> Enhanced Performance in Niche Tasks</h3>
<ul>
<li>Fine-tuned models can handle <strong>domain-specific tasks</strong> such as:
<ul>
<li>Legal document review</li>
<li>Medical record summarization</li>
<li>Technical support automation<br>
</li>
</ul></li>
<li>Achieves <strong>higher accuracy</strong> for these specialized tasks.</li>
</ul>
</section>
<section id="cost-optimization" class="level3" data-number="3.14.3">
<h3 data-number="3.14.3" class="anchored" data-anchor-id="cost-optimization"><span class="header-section-number">3.14.3</span> Cost Optimization</h3>
<ul>
<li>Fine-tuning a <strong>smaller, efficient model</strong> (e.g., gpt-4o-mini) can:
<ul>
<li>Reduce dependency on <strong>larger, costlier models</strong> like gpt-4o</li>
<li>Still achieve <strong>high accuracy</strong> for specialized use cases</li>
</ul></li>
</ul>
</section>
<section id="workflow-alignment" class="level3" data-number="3.14.4">
<h3 data-number="3.14.4" class="anchored" data-anchor-id="workflow-alignment"><span class="header-section-number">3.14.4</span> Workflow Alignment</h3>
<ul>
<li>Fine-tuning aligns the model‚Äôs behavior with:
<ul>
<li><strong>Domain-specific workflows</strong></li>
<li><strong>Data structures</strong></li>
<li><strong>Terminology</strong></li>
</ul></li>
<li>Ensures <strong>seamless integration</strong> into business processes</li>
</ul>
</section>
</section>
<section id="types-of-fine-tuning" class="level2" data-number="3.15">
<h2 data-number="3.15" class="anchored" data-anchor-id="types-of-fine-tuning"><span class="header-section-number">3.15</span> Types of Fine-Tuning</h2>
<section id="full-fine-tuning" class="level3" data-number="3.15.1">
<h3 data-number="3.15.1" class="anchored" data-anchor-id="full-fine-tuning"><span class="header-section-number">3.15.1</span> 1 Full Fine-Tuning</h3>
<p><strong>What it is:</strong><br>
- You train <strong>all the parameters</strong> of the model on your new dataset.<br>
- The model fully ‚Äúadapts‚Äù to the new task.</p>
<p><strong>Pros:</strong><br>
- Maximum flexibility ‚Äî can learn very specific tasks.<br>
- Can achieve high accuracy if you have lots of data.</p>
<p><strong>Cons:</strong><br>
- Very expensive in compute and memory, especially for large models (e.g., 70B LLaMA).<br>
- Slow to train.<br>
- Hard to store multiple fine-tuned versions (one per task).</p>
<p><strong>Example Use:</strong><br>
- Fine-tuning GPT-3 for a <strong>custom legal document assistant</strong>.</p>
<hr>
</section>
<section id="lora-low-rank-adaptation" class="level3" data-number="3.15.2">
<h3 data-number="3.15.2" class="anchored" data-anchor-id="lora-low-rank-adaptation"><span class="header-section-number">3.15.2</span> 2Ô∏è LoRA (Low-Rank Adaptation)</h3>
<p><strong>What it is:</strong><br>
- Only small additional matrices are trained, inserted into the model‚Äôs layers.<br>
- The <strong>original model weights stay frozen</strong>.</p>
<p><strong>Pros:</strong><br>
- Much lighter and faster than full fine-tuning.<br>
- Can fine-tune large models on a single GPU.<br>
- Easy to switch between multiple tasks (just load different LoRA weights).</p>
<p><strong>Cons:</strong><br>
- Might be slightly less flexible than full fine-tuning.</p>
<p><strong>Example Use:</strong><br>
- Fine-tuning LLaMA 3 for <strong>customer support chat</strong> using LoRA adapters.</p>
<hr>
</section>
<section id="qlora-quantized-lora" class="level3" data-number="3.15.3">
<h3 data-number="3.15.3" class="anchored" data-anchor-id="qlora-quantized-lora"><span class="header-section-number">3.15.3</span> 3Ô∏è QLoRA (Quantized LoRA)</h3>
<p><strong>What it is:</strong><br>
- A <strong>memory-efficient version of LoRA</strong>.<br>
- The base model is <strong>quantized</strong> (weights stored in lower precision, e.g., 4-bit or 8-bit).<br>
- LoRA adapters are still trained on top.</p>
<p><strong>Pros:</strong><br>
- Train very large models on a single GPU or limited hardware.<br>
- Maintains most of the original model‚Äôs performance.<br>
- Extremely cost-effective for research or experimentation.</p>
<p><strong>Cons:</strong><br>
- Slight precision loss due to quantization.</p>
<p><strong>Example Use:</strong><br>
- Fine-tuning a <strong>70B LLaMA model</strong> on your dataset using a 48GB GPU instead of a 128GB GPU.</p>
<hr>
</section>
<section id="prefix-tuning-or-p-tuning" class="level3" data-number="3.15.4">
<h3 data-number="3.15.4" class="anchored" data-anchor-id="prefix-tuning-or-p-tuning"><span class="header-section-number">3.15.4</span> 4Ô∏è Prefix Tuning (or P-Tuning)</h3>
<p><strong>What it is:</strong><br>
- Instead of changing model weights, you train only a set of <strong>‚Äúsoft prompts‚Äù</strong> that guide the model‚Äôs behavior.<br>
- The main model remains frozen.</p>
<p><strong>Pros:</strong><br>
- Extremely lightweight ‚Äî only a few thousand parameters.<br>
- Fast to train.<br>
- Ideal for models that are too large to fine-tune otherwise.</p>
<p><strong>Cons:</strong><br>
- Not as powerful as full fine-tuning or LoRA for very complex tasks.</p>
<p><strong>Example Use:</strong><br>
- Tuning GPT or LLaMA to <strong>answer FAQ questions in a specific style</strong> without touching the main model.</p>
</section>
</section>
<section id="full-fine-tuning-implementation" class="level2" data-number="3.16">
<h2 data-number="3.16" class="anchored" data-anchor-id="full-fine-tuning-implementation"><span class="header-section-number">3.16</span> Full Fine-Tuning Implementation</h2>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install tensorflow transformers datasets</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TFAutoModelForSequenceClassification, AutoTokenizer</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> create_optimizer</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>model_checkpoint <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>num_labels <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, from_pt <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_checkpoint)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"imdb"</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(examples[<span class="st">"text"</span>], padding<span class="op">=</span><span class="st">"max_length"</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>tokenized_datasets <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>tokenized_datasets</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>tf_train_dataset <span class="op">=</span> tokenized_datasets[<span class="st">"train"</span>].to_tf_dataset(</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">"input_ids"</span>, <span class="st">"attention_mask"</span>],</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    label_cols<span class="op">=</span>[<span class="st">"label"</span>],</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">16</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>tf_validation_dataset <span class="op">=</span> tokenized_datasets[<span class="st">"test"</span>].to_tf_dataset(</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">"input_ids"</span>, <span class="st">"attention_mask"</span>],</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    label_cols<span class="op">=</span>[<span class="st">"label"</span>],</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">16</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">5e-5</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>num_train_steps <span class="op">=</span> <span class="bu">len</span>(tokenized_datasets[<span class="st">"train"</span>]) <span class="op">//</span> batch_size <span class="op">*</span> num_epochs</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>optimizer, lr_schedule <span class="op">=</span> create_optimizer(</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    init_lr<span class="op">=</span>learning_rate,</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    num_train_steps<span class="op">=</span>num_train_steps,</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    weight_decay_rate<span class="op">=</span>weight_decay,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    num_warmup_steps<span class="op">=</span><span class="dv">0</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>optimizer,</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span>tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>tf.metrics.SparseCategoricalAccuracy()</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    tf_train_dataset,</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>tf_validation_dataset,</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>num_epochs</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating the performance of Fine-Tuned model against Pre-Trained model</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>model_checkpoint <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>num_labels <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels<span class="op">=</span>num_labels, from_pt <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>domain_specific_examples <span class="op">=</span> <span class="st">"The movie is fantastic"</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Domain-specific examples comparison:"</span>)</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">80</span>)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> loaded_tokenizer(domain_specific_examples, return_tensors<span class="op">=</span><span class="st">"tf"</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>outputs_pretrained <span class="op">=</span> model(inputs)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>preds_pretrained <span class="op">=</span> tf.nn.softmax(outputs_pretrained.logits, axis<span class="op">=-</span><span class="dv">1</span>).numpy()[<span class="dv">0</span>]</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>pred_class_pretrained <span class="op">=</span> np.argmax(preds_pretrained)</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>confidence_pretrained <span class="op">=</span> preds_pretrained[pred_class_pretrained]</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>outputs_finetuned <span class="op">=</span> loaded_model(inputs)</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>preds_finetuned <span class="op">=</span> tf.nn.softmax(outputs_finetuned.logits, axis<span class="op">=-</span><span class="dv">1</span>).numpy()[<span class="dv">0</span>]</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>pred_class_finetuned <span class="op">=</span> np.argmax(preds_finetuned)</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>confidence_finetuned <span class="op">=</span> preds_finetuned[pred_class_finetuned]</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Text: </span><span class="sc">{</span>domain_specific_examples<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Pre-trained model: Class </span><span class="sc">{</span>pred_class_pretrained<span class="sc">}</span><span class="ss"> with </span><span class="sc">{</span>confidence_pretrained<span class="sc">:.2f}</span><span class="ss"> confidence"</span>)</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fine-tuned model: Class </span><span class="sc">{</span>pred_class_finetuned<span class="sc">}</span><span class="ss"> with </span><span class="sc">{</span>confidence_finetuned<span class="sc">:.2f}</span><span class="ss"> confidence"</span>)</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">80</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<footer class="qmd-watermark-footer">
¬© 2025 Arundev Vamadevan ‚Äî For Mar Baselios FDP participants only. All rights reserved.
</footer>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>